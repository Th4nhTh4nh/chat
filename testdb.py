import streamlit as st
import streamlit.components.v1 as components
import google.generativeai as genai
import os
import uuid
import re
import chromadb
from chromadb import EmbeddingFunction
from pypdf import PdfReader
from docx import Document
import pandas as pd
from dotenv import load_dotenv
import tempfile
import time
from datetime import datetime
import hashlib
import logging
from sqlite import *

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

# C·∫•u h√¨nh v√† h·∫±ng s·ªë
DEFAULT_CHUNK_SIZE = 1000
DEFAULT_CHUNK_OVERLAP = 200
DEFAULT_NUM_RESULTS = 3
TEMPERATURE = 0.2


# RAG
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input):
        try:
            gemini_api_key = os.getenv("API_KEY")
            if not gemini_api_key:
                raise ValueError("Missing API key")
            genai.configure(api_key=gemini_api_key)
            model = "models/embedding-001"
            title = "Document Embedding"

            results = []
            for text in input:
                try:
                    embedding = genai.embed_content(
                        model=model,
                        content=text,
                        task_type="retrieval_document",
                        title=title
                    )["embedding"]
                    results.append(embedding)
                    time.sleep(0.1)
                except Exception as e:
                    logger.error(f"Error embedding text: {str(e)}")
                    results.append([0.0] * 768)
            return results
        except Exception as e:
            logger.error(f"Embedding function error: {str(e)}")
            return [[0.0] * 768 for _ in range(len(input))]


# X·ª≠ l√Ω nhi·ªÅu lo·∫°i t√†i li·ªáu
def extract_text_from_file(file_path, file_type):
    try:
        if file_type == 'pdf':
            return extract_text_from_pdf(file_path)
        elif file_type == 'docx':
            return extract_text_from_docx(file_path)
        elif file_type == 'txt':
            return extract_text_from_txt(file_path)
        elif file_type == 'csv':
            return extract_text_from_csv(file_path)
        else:
            return f"Unsupported file type: {file_type}"
    except Exception as e:
        logger.error(f"Error extracting text: {str(e)}")
        return f"Error processing file: {str(e)}"


def extract_text_from_pdf(file_path):
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n\n"
    return text


def extract_text_from_docx(file_path):
    doc = Document(file_path)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n\n"
    return text


def extract_text_from_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
        return f.read()


def extract_text_from_csv(file_path):
    df = pd.read_csv(file_path)
    return df.to_string(index=False)


# Ph∆∞∆°ng ph√°p chunking
def split_text_into_chunks(text, chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP):
    """Split text into overlapping chunks of specified size."""
    chunks = []

    # Ph√¢n chia ƒëo·∫°n vƒÉn c∆° b·∫£n ƒë·∫ßu ti√™n
    paragraphs = re.split(r'\n\s*\n', text)
    current_chunk = ""

    for paragraph in paragraphs:
        # B·ªè qua vƒÉn b·∫£n tr·ªëng
        if not paragraph.strip():
            continue

        # N·∫øu vi·ªác th√™m ƒëo·∫°n vƒÉn n√†y v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc chunk, l∆∞u chunk hi·ªán t·∫°i v√† b·∫Øt ƒë·∫ßu m·ªôt chunk m·ªõi
        if len(current_chunk) + len(paragraph) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            # Gi·ªØ l·∫°i m·ªôt s·ªë ph·∫ßn chunk ch·ªìng ch√©o t·ª´ ph·∫ßn chunk tr∆∞·ªõc
            overlap_size = min(chunk_overlap, len(current_chunk))
            current_chunk = current_chunk[-overlap_size:] if overlap_size > 0 else ""

        # Th√™m ƒëo·∫°n vƒÉn v√†o chunk hi·ªán t·∫°i
        current_chunk += paragraph + "\n\n"

    # Th√™m chunk c√≤n l·∫°i cu·ªëi c√πng n·∫øu c√≥
    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    # N·∫øu ch√∫ng ta kh√¥ng c√≥ chunk n√†o (v√≠ d·ª•: t√†i li·ªáu r·∫•t ng·∫Øn), h√£y tr·∫£ v·ªÅ to√†n b·ªô vƒÉn b·∫£n d∆∞·ªõi d·∫°ng m·ªôt chunk
    if not chunks:
        chunks = [text.strip()]

    # Th√™m metadata v√†o chunks (e.g., v·ªã tr√≠ trong t√†i li·ªáu)
    enhanced_chunks = []
    for i, chunk in enumerate(chunks):
        # Th√™m index d∆∞·ªõi d·∫°ng metadata
        chunk_with_metadata = {
            "text": chunk,
            "metadata": {
                "chunk_id": i,
                "position": f"{i}/{len(chunks)}",
                "char_count": len(chunk)
            }
        }
        enhanced_chunks.append(chunk_with_metadata)

    return enhanced_chunks


def create_chroma_collection(document_chunks, collection_name):
    """Create a ChromaDB collection from document chunks with metadata."""
    chroma_dir = "./chroma_db"
    os.makedirs(chroma_dir, exist_ok=True)

    chroma_client = chromadb.PersistentClient(path=chroma_dir)

    # Ki·ªÉm tra xem c√≥ t·ªìn t·∫°i kh√¥ng, n·∫øu c√≥, h√£y x√≥a n√≥ ƒë·ªÉ t·∫°o l·∫°i
    try:
        existing_collection = chroma_client.get_collection(name=collection_name)
        chroma_client.delete_collection(name=collection_name)
    except Exception:
        pass

    # T·∫°o m·ªõi
    collection = chroma_client.create_collection(
        name=collection_name,
        embedding_function=GeminiEmbeddingFunction()
    )

    # Tr√≠ch xu·∫•t vƒÉn b·∫£n v√† metadata t·ª´ chunk
    texts = [chunk["text"] for chunk in document_chunks]
    metadatas = [chunk["metadata"] for chunk in document_chunks]
    ids = [f"chunk_{i}" for i in range(len(document_chunks))]

    # Th√™m t√†i li·ªáu theo t·ª´ng ƒë·ª£t ƒë·ªÉ tr√°nh c√°c gi·ªõi h·∫°n
    batch_size = 20
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_metadatas = metadatas[i:i + batch_size]
        batch_ids = ids[i:i + batch_size]

        try:
            collection.add(
                documents=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids
            )
        except Exception as e:
            logger.error(f"Error adding batch to collection: {str(e)}")

    return collection, collection_name


def load_chroma_collection(collection_name):
    chroma_dir = "./chroma_db"
    chroma_client = chromadb.PersistentClient(path=chroma_dir)
    return chroma_client.get_collection(
        name=collection_name,
        embedding_function=GeminiEmbeddingFunction()
    )


def get_relevant_passages(query, collection, n_results=DEFAULT_NUM_RESULTS):
    """Truy xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan v√† metadata c·ªßa ch√∫ng."""
    try:
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        # K·∫øt h·ª£p c√°c t√†i li·ªáu v·ªõi metadata c·ªßa ch√∫ng ƒë·ªÉ c√≥ ng·ªØ c·∫£nh t·ªët h∆°n
        passages = []
        for i, doc in enumerate(results['documents'][0]):
            metadata = results['metadatas'][0][i]
            relevance = 1.0 - min(results['distances'][0][i] / 2.0, 0.99)  # Chu·∫©n h√≥a kho·∫£ng c√°ch ƒë·∫øn ƒëi·ªÉm li√™n quan

            passages.append({
                "text": doc,
                "metadata": metadata,
                "relevance": f"{relevance:.2f}"
            })

        return passages
    except Exception as e:
        logger.error(f"Error retrieving passages: {str(e)}")
        return []


def make_rag_prompt(query, relevant_passages):
    """T·∫°o prompt g·ªìm c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan v√† metadata c·ªßa ch√∫ng."""
    # ƒê·ªãnh d·∫°ng ƒëo·∫°n vƒÉn v·ªõi metadata c·ªßa ch√∫ng
    formatted_passages = []

    for i, passage in enumerate(relevant_passages):
        formatted_passage = f"[Passage {i + 1} - Relevance: {passage['relevance']}]\n{passage['text']}"
        formatted_passages.append(formatted_passage)

    context = "\n\n".join(formatted_passages)

    prompt = """You are a helpful and informative assistant that answers questions based on the provided document passages. 
    Follow these guidelines:

    1. Answer using information from the provided passages
    2. If the answer isn't contained in the passages, say "The document doesn't contain information about that."
    3. Cite specific parts of the passages to support your answer
    4. Respond in a conversational and helpful tone
    5. Break down complex concepts for non-technical users
    6. Keep your answer concise but comprehensive

    QUESTION: {query}

    RELEVANT PASSAGES:
    {context}

    ANSWER:
    """.format(query=query, context=context)

    return prompt


# C·∫•u h√¨nh Gemini API
def configure_gemini_api():
    genai.configure(api_key=os.getenv("API_KEY"))
    model = genai.GenerativeModel(
        os.getenv("MODEL_NAME"),
        generation_config={"temperature": TEMPERATURE}
    )
    return model


# Ghi nh·ªõ ng·ªØ c·∫£nh h·ªôi tho·∫°i
def multi_conversation(model, conv_id=None):
    if conv_id is None or conv_id not in st.session_state.chats:
        chat = model.start_chat(history=[])
        if conv_id:
            st.session_state.chats[conv_id] = chat
        return chat
    return st.session_state.chats[conv_id]


# T·∫°o c√¢u tr·∫£ l·ªùi k·∫øt h·ª£p RAG
def generate_response(chat, prompt, file=None, use_rag=False, history=None):
    try:
        if history and len(history) > 0:
            recent_history = history[-5:]
            for msg in recent_history:
                if msg["role"] == "user":
                    chat.send_message(msg["content"])
        start_time = time.time()

        if use_rag and "current_collection" in st.session_state:
            # D√πng RAG ƒë t·∫°o ph·∫£n h·ªìi
            collection = st.session_state.current_collection

            # L·∫•y c√°c th√¥ng tin li√™n quan
            relevant_passages = get_relevant_passages(
                prompt,
                collection,
                n_results=st.session_state.get("num_results", DEFAULT_NUM_RESULTS)
            )

            # L∆∞u c√°c ƒëo·∫°n vƒÉn ƒë√£ l·∫•y v√†o session state ƒë·ªÉ hi·ªÉn th·ªã
            st.session_state.last_retrieved_passages = relevant_passages

            # T·∫°o prompt RAG
            rag_prompt = make_rag_prompt(prompt, relevant_passages)

            # Gen ra ph·∫£n h·ªìi
            response = chat.send_message(rag_prompt)

            # Th·ªùi gian x·ª≠ l√Ω
            processing_time = time.time() - start_time
            logger.info(f"RAG response generated in {processing_time:.2f} seconds")

            return response.text

        elif file:
            # X·ª≠ l√Ω t·ªáp kh√¥ng d√πng RAG b·∫±ng ph∆∞∆°ng ph√°p ban ƒë·∫ßu
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file.name.split(".")[-1]}') as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file_path = tmp_file.name

            with open(tmp_file_path, 'rb') as f:
                file_content = f.read()

            # ƒê∆∞·ªùng d·∫´n file
            file_part = {"mime_type": f"application/{file.name.split('.')[-1]}", "data": file_content}

            # G·ª≠i tin nh·∫Øn v·ªõi c·∫£ vƒÉn b·∫£n v√† file
            response = chat.send_message([prompt, file_part])

            # X√≥a file l∆∞u t·∫°m th·ªùi
            os.unlink(tmp_file_path)

            return response.text

        else:
            response = chat.send_message(prompt)
            return response.text

    except Exception as e:
        logger.error(f"Error generating response: {str(e)}")
        return f"I encountered an error while generating a response: {str(e)}"


# def display_message(role, content):
#     with st.chat_message(role):
#         st.write(content)

def display_message(role, content):
    if role == "user":
        st.markdown(
            f"<div style='background-color: #E3F2FD; padding: 10px; border-radius: 8px; margin: 5px 0;'>{content}</div>",
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            f"<div style='background-color: #F5F5F5; padding: 10px; border-radius: 8px; margin: 5px 0;'>{content}</div>",
            unsafe_allow_html=True
        )


def type_writer_effect(text, speed=0.003):
    container = st.empty()
    displayed_text = ""

    for char in text:
        displayed_text += char
        container.markdown(displayed_text)
        time.sleep(speed)

    return container


# L∆∞u tr·ªØ t√†i li·ªáu
def save_document_metadata(filename, file_type, collection_name=None):
    """L∆∞u metadata v·ªÅ t√†i li·ªáu ƒë√£ t·∫£i l√™n."""
    if "documents" not in st.session_state:
        st.session_state.documents = {}

    doc_id = hashlib.md5(f"{filename}_{datetime.now()}".encode()).hexdigest()

    st.session_state.documents[doc_id] = {
        "filename": filename,
        "type": file_type,
        "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "collection_name": collection_name,
    }

    return doc_id


# L∆∞u h·ªôi tho·∫°i
def save_conversation(title, messages, conv_id=None):
    if "conversations" not in st.session_state:
        st.session_state.conversations = {}

    if not conv_id:
        conv_id = str(uuid.uuid4())

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    st.session_state.conversations[conv_id] = {
        "title": title,
        "messages": messages.copy(),
        "timestamp": timestamp,
        "rag_enabled": st.session_state.get("rag_enabled", False),
        "collection_name": st.session_state.get("collection_name", None),
        "document_id": st.session_state.get("current_document_id", None),
        "rag_settings": {
            "chunk_size": st.session_state.get("chunk_size", DEFAULT_CHUNK_SIZE),
            "chunk_overlap": st.session_state.get("chunk_overlap", DEFAULT_CHUNK_OVERLAP),
            "num_results": st.session_state.get("num_results", DEFAULT_NUM_RESULTS),
        }
    }

    return conv_id


# Upload v√† x·ª≠ l√Ω t√†i li·ªáu
def upload_and_process_document():
    st.header("Upload Document")

    supported_types = ["pdf", "docx", "txt", "csv"]
    uploaded_file = st.file_uploader(
        "Upload a document to chat with",
        type=supported_types
    )

    if uploaded_file:
        file_type = uploaded_file.name.split(".")[-1].lower()
        st.success(f"{uploaded_file.name} uploaded successfully")

        # Hi·ªÉn th·ªã c√†i ƒë·∫∑t RAG n·∫øu c√≥ file ƒë∆∞·ª£c t·∫£i l√™n
        with st.expander("RAG Settings"):
            col1, col2, col3 = st.columns(3)
            with col1:
                chunk_size = st.number_input(
                    "Chunk Size",
                    min_value=100,
                    max_value=2000,
                    value=st.session_state.get("chunk_size", DEFAULT_CHUNK_SIZE)
                )
            with col2:
                chunk_overlap = st.number_input(
                    "Chunk Overlap",
                    min_value=0,
                    max_value=500,
                    value=st.session_state.get("chunk_overlap", DEFAULT_CHUNK_OVERLAP)
                )
            with col3:
                num_results = st.number_input(
                    "Results to Retrieve",
                    min_value=1,
                    max_value=10,
                    value=st.session_state.get("num_results", DEFAULT_NUM_RESULTS)
                )

            # L∆∞u c√†i ƒë·∫∑t v√†o session state
            st.session_state.chunk_size = chunk_size
            st.session_state.chunk_overlap = chunk_overlap
            st.session_state.num_results = num_results

        col1, col2 = st.columns(2)

        with col1:
            if st.button("Process with RAG"):
                with st.spinner("Processing document... This may take a minute"):
                    # L∆∞u t·ªáp t·∫£i l√™n nh∆∞ 1 file t·∫°m th·ªùi
                    with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_file_path = tmp_file.name

                    # Truy xu·∫•t vƒÉn b·∫£n t·ª´ t√†i li·ªáu
                    document_text = extract_text_from_file(tmp_file_path, file_type)

                    # X√≥a file t·∫°m
                    os.unlink(tmp_file_path)

                    if not document_text or len(document_text) < 10:
                        st.error("Could not extract text from document. Please try another file.")
                        return uploaded_file

                    # Chia nh·ªè vƒÉn b·∫£n th√†nh chunk v·ªõi metadata
                    text_chunks = split_text_into_chunks(
                        document_text,
                        chunk_size=st.session_state.chunk_size,
                        chunk_overlap=st.session_state.chunk_overlap
                    )

                    # T·∫°o t√™n collection duy nh·∫•t
                    collection_name = f"{file_type}_{datetime.now().strftime('%Y%m%d%H%M%S')}"

                    # T·∫°o collection
                    collection, _ = create_chroma_collection(text_chunks, collection_name)

                    # L∆∞u metadata c·ªßa t√†i li·ªáu
                    doc_id = save_document_metadata(uploaded_file.name, file_type, collection_name)

                    # L∆∞u tr·ªØ v√†o session state
                    st.session_state.current_collection = collection
                    st.session_state.collection_name = collection_name
                    st.session_state.rag_enabled = True
                    st.session_state.current_document_id = doc_id

                    st.success(f"Document processed with {len(text_chunks)} chunks. Ready for questions!")

        with col2:
            if st.button("Use Standard Mode"):
                st.session_state.rag_enabled = False
                st.session_state.pop("current_collection", None)
                st.session_state.pop("collection_name", None)
                st.success("Using standard mode with document")

    return uploaded_file


# Hi·ªÉn th·ªã ph√¢n t√≠ch t√†i li·ªáu
def document_analysis():
    if "last_retrieved_passages" in st.session_state and st.session_state.rag_enabled:
        st.subheader("Retrieved Passages")

        passages = st.session_state.last_retrieved_passages

        for i, passage in enumerate(passages):
            with st.expander(f"Passage {i + 1} - Relevance: {passage['relevance']}"):
                st.markdown(passage["text"])
                st.caption(f"Position: {passage['metadata'].get('position', 'Unknown')}")


# Qu·∫£n l√Ω t√†i li·ªáu
def document_management():
    if "documents" in st.session_state and st.session_state.documents:
        st.subheader("Document Library")

        for doc_id, doc_data in st.session_state.documents.items():
            col1, col2, col3 = st.columns([3, 1, 1])

            with col1:
                st.write(f"üìÑ {doc_data['filename']}")

            with col2:
                st.caption(f"Uploaded: {doc_data['upload_time']}")

            with col3:
                if doc_data.get("collection_name") and st.button("Use", key=f"use_{doc_id}"):
                    try:
                        collection = load_chroma_collection(doc_data["collection_name"])
                        st.session_state.current_collection = collection
                        st.session_state.collection_name = doc_data["collection_name"]
                        st.session_state.current_document_id = doc_id
                        st.session_state.rag_enabled = True
                        st.success(f"Now using {doc_data['filename']} for RAG")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Could not load document: {str(e)}")


def main():
    st.set_page_config(
        page_title="L·ªèser CHat",
        page_icon="üìö",
        # layout="wide"
    )

    with open("static/styles.css", mode="r") as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

    st.markdown("<h1 style='font-size: 32px; color: #333;'>L·ªèe CHat</h1>", unsafe_allow_html=True)
    # Kh·ªüi t·∫°o session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_conversation" not in st.session_state:
        st.session_state.current_conversation = None
    if "conversations" not in st.session_state:
        st.session_state.conversations = {}
    if "chats" not in st.session_state:
        st.session_state.chats = {}
    if "rag_enabled" not in st.session_state:
        st.session_state.rag_enabled = False
    if "chunk_size" not in st.session_state:
        st.session_state.chunk_size = DEFAULT_CHUNK_SIZE
    if "chunk_overlap" not in st.session_state:
        st.session_state.chunk_overlap = DEFAULT_CHUNK_OVERLAP
    if "num_results" not in st.session_state:
        st.session_state.num_results = DEFAULT_NUM_RESULTS

    # Kh·ªüi t·∫°o m√¥ h√¨nh
    model = configure_gemini_api()

    # Layout v·ªõi sidebar v√† khu v·ª±c ch√≠nh
    # col_sidebar, col_main = st.columns([1, 3])

    # Sidebar
    # with col_sidebar:
    with st.sidebar:
        # Tr·∫°ng th√°i RAG
        if st.session_state.rag_enabled:
            st.success("üìö RAG Mode: Enabled")

            if "current_document_id" in st.session_state and st.session_state.current_document_id:
                doc_data = st.session_state.documents.get(st.session_state.current_document_id, {})
                if doc_data:
                    st.info(f"Using document: {doc_data.get('filename', 'Unknown')}")
        else:
            st.info("RAG Mode: Disabled")

        # N√∫t t·∫°o h·ªôi tho·∫°i m·ªõi
        if st.button("New Conversation"):
            st.session_state.messages = []
            st.session_state.current_conversation = None
            st.rerun()

        # Tab cho l·ªãch s·ª≠ v√† qu·∫£n l√Ω t√†i li·ªáu
        tabs = st.tabs(["Conversations", "Documents", "Settings"])
        with tabs[0]:
            # Hi·ªÉn th·ªã c√°c cu·ªôc tr√≤ chuy·ªán c≈©
            if st.session_state.conversations:
                for conv_id, conv_data in st.session_state.conversations.items():
                    # Hi·ªÉn th·ªã th√¥ng b√°o RAG cho c√°c cu·ªôc tr√≤ chuy·ªán v·ªõi RAG
                    rag_indicator = "üîç " if conv_data.get("rag_enabled", False) else ""
                    if st.button(f"{rag_indicator}{conv_data['title']}", key=f"conv_{conv_id}"):
                        st.session_state.messages = conv_data["messages"].copy()
                        st.session_state.current_conversation = conv_id

                        # Kh√¥i ph·ª•c c√†i ƒë·∫∑t RAG n·∫øu c√≥ th·ªÉ
                        if conv_data.get("rag_enabled", False) and conv_data.get("collection_name"):
                            try:
                                st.session_state.current_collection = load_chroma_collection(
                                    conv_data["collection_name"])
                                st.session_state.collection_name = conv_data["collection_name"]
                                st.session_state.rag_enabled = True

                                # Kh√¥i ph·ª•c c√†i ƒë·∫∑t RAG
                                rag_settings = conv_data.get("rag_settings", {})
                                st.session_state.chunk_size = rag_settings.get("chunk_size", DEFAULT_CHUNK_SIZE)
                                st.session_state.chunk_overlap = rag_settings.get("chunk_overlap",
                                                                                  DEFAULT_CHUNK_OVERLAP)
                                st.session_state.num_results = rag_settings.get("num_results", DEFAULT_NUM_RESULTS)

                                # Kh√¥i ph·ª•c t√†i li·ªáu tham kh·∫£o
                                st.session_state.current_document_id = conv_data.get("document_id")
                            except Exception as e:
                                st.error(f"Could not load RAG data: {str(e)}")
                                st.session_state.rag_enabled = False
                        else:
                            st.session_state.rag_enabled = False

                        st.rerun()
            else:
                st.write("No conversations yet.")

        with tabs[1]:
            # Qu·∫£n l√Ω t√†i li·ªáu
            document_management()

        with tabs[2]:
            # C√†i ƒë·∫∑t
            st.subheader("AI Settings")
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=1.0,
                value=TEMPERATURE,
                step=0.1,
                help="Higher values make output more random, lower values more deterministic"
            )

    uploaded_file = upload_and_process_document()

    # Giao di·ªán chat
    st.markdown("<h2 style='font-size: 24px; color: #555; margin-top: 20px;'>Chat</h2>", unsafe_allow_html=True)

    # Hi·ªÉn th·ªã l·ªãch s·ªß chat
    for message in st.session_state.messages:
        display_message(message["role"], message["content"])

    # Hi·ªÉn th·ªã ph√¢n t√≠ch t√†i li·ªáu
    document_analysis()

    # Khu v·ª±c nh·∫≠p tin nh·∫Øn m·ªõi
    user_input = st.chat_input("Enter your message...")

    if user_input:
        # Th√™m tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "user", "content": user_input})
        display_message("user", user_input)

        # Nh·∫≠n chat hi·ªán t·∫°i ho·∫∑c t·∫°o chat m·ªõi
        current_chat = multi_conversation(model, st.session_state.current_conversation)

        with st.spinner("Thinking..."):
            # Gen ra ph·∫£n h·ªìi
            if uploaded_file and not st.session_state.rag_enabled:
                # X·ª≠ l√Ω file m√† ko d√πng RAG
                response = generate_response(
                    current_chat,
                    user_input,
                    file=uploaded_file,
                    history=st.session_state.messages)
            else:
                # X·ª≠ l√Ω b·∫±ng RAG n·∫øu ƒë∆∞·ª£c b·∫≠t ho·∫∑c ch·ªâ vƒÉn b·∫£n n·∫øu kh√¥ng
                response = generate_response(
                    current_chat,
                    user_input,
                    use_rag=st.session_state.rag_enabled,
                    history=st.session_state.messages
                )

            # Th√™m ph·∫£n h·ªìi v√†o m·ª•c l·ªãch s·ª≠
            st.session_state.messages.append({"role": "assistant", "content": response})
            display_message("assistant", response)

            # HI·ªáu ·ª©ng khi ph·∫£n h·ªìi
            # st.session_state.messages.append({"role": "assistant", "content": response})
            # with st.chat_message("assistant"):
            #     type_writer_effect(response)

            # L∆∞u h·ªôi tho·∫°i
            if len(st.session_state.messages) >= 2:
                # T·∫°o ti√™u ƒë·ªÅ t·ª´ tin nh·∫Øn ƒë·∫ßu ti√™n c·ªßa chat
                first_message = next((msg for msg in st.session_state.messages if msg["role"] == "user"), None)
                if first_message:
                    title = first_message["content"][:30] + "..." if len(first_message["content"]) > 30 else \
                        first_message["content"]
                else:
                    title = f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M')}"

                # L∆∞u
                conv_id = save_conversation(title, st.session_state.messages, st.session_state.current_conversation)
                st.session_state.current_conversation = conv_id

    # Khu v·ª±c hi·ªÉn th·ªã content
    # with col_main:
    # T·∫£i file l√™n
    # uploaded_file = upload_and_process_document()
    #
    # # Giao di·ªán chat
    # st.markdown("<h2 style='font-size: 24px; color: #555; margin-top: 20px;'>Chat</h2>", unsafe_allow_html=True)
    #
    # # Hi·ªÉn th·ªã l·ªãch s·ªß chat
    # for message in st.session_state.messages:
    #     display_message(message["role"], message["content"])
    #
    # # Hi·ªÉn th·ªã ph√¢n t√≠ch t√†i li·ªáu
    # document_analysis()
    #
    # # Khu v·ª±c nh·∫≠p tin nh·∫Øn m·ªõi
    # user_input = st.chat_input("Enter your message...")
    #
    # if user_input:
    #     # Th√™m tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ chat
    #     st.session_state.messages.append({"role": "user", "content": user_input})
    #     display_message("user", user_input)
    #
    #     # Nh·∫≠n chat hi·ªán t·∫°i ho·∫∑c t·∫°o chat m·ªõi
    #     current_chat = multi_conversation(model, st.session_state.current_conversation)
    #
    #     with st.spinner("Thinking..."):
    #         # Gen ra ph·∫£n h·ªìi
    #         if uploaded_file and not st.session_state.rag_enabled:
    #             # X·ª≠ l√Ω file m√† ko d√πng RAG
    #             response = generate_response(
    #                 current_chat,
    #                 user_input,
    #                 file=uploaded_file,
    #                 history=st.session_state.messages)
    #         else:
    #             # X·ª≠ l√Ω b·∫±ng RAG n·∫øu ƒë∆∞·ª£c b·∫≠t ho·∫∑c ch·ªâ vƒÉn b·∫£n n·∫øu kh√¥ng
    #             response = generate_response(
    #                 current_chat,
    #                 user_input,
    #                 use_rag=st.session_state.rag_enabled,
    #                 history=st.session_state.messages
    #             )
    #
    #         # Th√™m ph·∫£n h·ªìi v√†o m·ª•c l·ªãch s·ª≠
    #         st.session_state.messages.append({"role": "assistant", "content": response})
    #         display_message("assistant", response)
    #
    #         # st.session_state.messages.append({"role": "assistant", "content": response})
    #         # with st.chat_message("assistant"):
    #         #     type_writer_effect(response)
    #
    #         # L∆∞u h·ªôi tho·∫°i
    #         if len(st.session_state.messages) >= 2:
    #             # T·∫°o ti√™u ƒë·ªÅ t·ª´ tin nh·∫Øn ƒë·∫ßu ti√™n c·ªßa chat
    #             first_message = next((msg for msg in st.session_state.messages if msg["role"] == "user"), None)
    #             if first_message:
    #                 title = first_message["content"][:30] + "..." if len(first_message["content"]) > 30 else \
    #                     first_message["content"]
    #             else:
    #                 title = f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    #
    #             # L∆∞u
    #             conv_id = save_conversation(title, st.session_state.messages, st.session_state.current_conversation)
    #             st.session_state.current_conversation = conv_id


if __name__ == "__main__":
    main()
